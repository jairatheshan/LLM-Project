{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcb14f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E_Jairath\\OneDrive - Domino Printing Sciences\\Desktop\\Base\\Project Work\\LLM Project\\LLM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\E_Jairath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import yaml\n",
    "import nltk\n",
    "import uuid\n",
    "import openai\n",
    "import os\n",
    "import chromadb\n",
    "\n",
    "from chromadb.config import Settings\n",
    "from openai import AzureOpenAI  #\n",
    "from chromadb import PersistentClient\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "from openai import AzureOpenAI  # Make sure this import is correct\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bcd5f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to locate 'punkt'...\n",
      "‚ùå 'punkt' not found. Downloading now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\E_Jairath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(\"Trying to locate 'punkt'...\")\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"‚úÖ 'punkt' is installed correctly.\")\n",
    "except LookupError:\n",
    "    print(\"‚ùå 'punkt' not found. Downloading now...\")\n",
    "    nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d875816",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = \"API_Cred-copy.yaml\"\n",
    "\n",
    "with open(yaml_file_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "# openai\n",
    "API_KEY = config['Open_ai_credentails']['API_KEY']\n",
    "RESOURCE_ENDPOINT = config['Open_ai_credentails']['RESOURCE_ENDPOINT']\n",
    "MODEL = config['Open_ai_credentails']['MODEL']\n",
    "API_VERSION = config['Open_ai_credentails']['API_VERSION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2889d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Extract text from PDF ---\n",
    "def extract_text_from_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "# --- Step 2: Chunk the text into manageable pieces ---\n",
    "def split_text(text, max_chars=500, overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + max_chars\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += max_chars - overlap\n",
    "    return chunks\n",
    "\n",
    "def smart_split_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "\n",
    "def semantic_split_text(text, max_chars=500, overlap=50):\n",
    "    from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(chunk) + len(sentence) <= max_chars:\n",
    "            chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = sentence\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "# --- Step 3: Embed the chunks using SentenceTransformer ---\n",
    "def embed_chunks(chunks, model_name='all-MiniLM-L6-v2'):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(chunks, convert_to_numpy=True)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def store_in_chromadb(chunks, embeddings, db_path=\"./chroma_store\"):\n",
    "    # ‚úÖ Updated client\n",
    "    client = PersistentClient(path=db_path)\n",
    "\n",
    "    # Get or create a collection\n",
    "    collection = client.get_or_create_collection(\"pdf_docs\")\n",
    "\n",
    "    # Add data to the collection\n",
    "    collection.add(\n",
    "        documents=chunks,\n",
    "        embeddings=[e.tolist() for e in embeddings],\n",
    "        metadatas=[{\"source\": \"my_pdf\"} for _ in chunks],\n",
    "        ids=[str(uuid.uuid4()) for _ in chunks]\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Successfully stored into ChromaDB (new API)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff242caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extracting text...\n",
      "‚úÇÔ∏è Splitting into chunks...\n",
      "üß† Embedding chunks...\n",
      "üíæ Storing into ChromaDB...\n",
      "‚úÖ Successfully stored into ChromaDB (new API)\n",
      "üöÄ Done.\n"
     ]
    }
   ],
   "source": [
    "# --- MAIN PIPELINE ---\n",
    "pdf_path = \"Documentation of PPI (Customer useage patterns).pdf\"  \n",
    "\n",
    "print(\"üîç Extracting text...\")\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(\"‚úÇÔ∏è Splitting into chunks...\")\n",
    "chunks = semantic_split_text(text)\n",
    "\n",
    "print(\"üß† Embedding chunks...\")\n",
    "embeddings = embed_chunks(chunks)\n",
    "\n",
    "print(\"üíæ Storing into ChromaDB...\")\n",
    "store_in_chromadb(chunks, embeddings)\n",
    "\n",
    "print(\"üöÄ Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdefea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your persisted Chroma vector database\n",
    "client = PersistentClient(path=\"./chroma_store\")\n",
    "# Access the collection you previously created\n",
    "collection = client.get_collection(\"pdf_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "474b1349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top matching chunks:\n",
      "\n",
      "üìÑ Source: my_pdf\n",
      "üß† Chunk:\n",
      "Printer Performance Insights  \n",
      "Customer Consumables Prediction  \n",
      " \n",
      "Revision: 1  \n",
      "Version: 1 \n",
      "Document Number: Doc-000XXXX  \n",
      "Author:  \n",
      " \n",
      " \n",
      "Document Type: Technical report \n",
      " \n",
      " \n",
      "The Work In Progress watermark must be switched \n",
      "Based on Template: Doc-0011608 \n",
      " \n",
      " \n",
      "on while this document is being edited. DO NOT EDIT THIS BOX \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Contents \n",
      "1.\n",
      "--------------------------------------------------\n",
      "üìÑ Source: my_pdf\n",
      "üß† Chunk:\n",
      "Printer Performance Insights  \n",
      "Customer Consumables Prediction  \n",
      " \n",
      "Revision: 1  \n",
      "Version: 1 \n",
      "Document Number: Doc-000XXXX  \n",
      "Author:  \n",
      " \n",
      " \n",
      "Document Type: Technical report \n",
      " \n",
      " \n",
      "The Work In Progress watermark must be switched \n",
      "Based on Template: Doc-0011608 \n",
      " \n",
      " \n",
      "on while this document is being edited. DO NOT EDIT THIS BOX \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Contents \n",
      "1.\n",
      "--------------------------------------------------\n",
      "üìÑ Source: my_pdf\n",
      "üß† Chunk:\n",
      "Printer Performance Insights  \n",
      "Customer Consumables Prediction  \n",
      " \n",
      "Revision: 1  \n",
      "Version: 1 \n",
      "Document Number: Doc-000XXXX  \n",
      "Author:  \n",
      " \n",
      " \n",
      "Document Type: Technical report \n",
      " \n",
      " \n",
      "The Work In Progress watermark must be switched \n",
      "Based on Template: Doc-0011608 \n",
      " \n",
      " \n",
      "on while this document is being edited. DO NOT EDIT THIS BOX \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Contents \n",
      "1.\n",
      "--------------------------------------------------\n",
      "üìÑ Source: my_pdf\n",
      "üß† Chunk:\n",
      "Printer Performance Insights  \n",
      "Customer Consumables Prediction  \n",
      " \n",
      "Revision: 1  \n",
      "Version: 1 \n",
      "Document Number: Doc-000XXXX  \n",
      "Author:  \n",
      " \n",
      " \n",
      "Document Type: Technical report \n",
      " \n",
      " \n",
      "The Work In Progress watermark must be switched \n",
      "Based on Template: Doc-0011608 \n",
      " \n",
      " \n",
      "on while this document is being edited. DO NOT EDIT THIS BOX \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Contents \n",
      "1.\n",
      "--------------------------------------------------\n",
      "üìÑ Source: my_pdf\n",
      "üß† Chunk:\n",
      "Printer Performance Insights  \n",
      "Customer Consumables Prediction  \n",
      " \n",
      "Revision: 1  \n",
      "Version: 1 \n",
      "Document Number: Doc-000XXXX  \n",
      "Author:  \n",
      " \n",
      " \n",
      "Document Type: Technical report \n",
      " \n",
      " \n",
      "The Work In Progress watermark must be switched \n",
      "Based on Template: Doc-0011608 \n",
      " \n",
      " \n",
      "on while this document is being edited. DO NOT EDIT THIS BOX \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Contents \n",
      "1.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Embed a user query using the same model used for indexing\n",
    "embedding_model = 'multi-qa-MiniLM-L6-cos-v1' #fast, good for Q&A\n",
    "model = SentenceTransformer(embedding_model)\n",
    "query = \"what is this document about ?\"\n",
    "query_embedding = model.encode([query])[0].tolist()\n",
    "\n",
    "# Search the ChromaDB with that vector\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=5  # top 3 most relevant chunks\n",
    ")\n",
    "\n",
    "# Print out the results\n",
    "print(\"Top matching chunks:\\n\")\n",
    "for doc, metadata in zip(results['documents'][0], results['metadatas'][0]):\n",
    "    print(f\"üìÑ Source: {metadata['source']}\\nüß† Chunk:\\n{doc}\\n{'-'*50}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c6c0564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Answer: \n",
      "The calculation of unique makeup cartridges for each customer is done through the following procedure:\n",
      "\n",
      "1. **Data Filtering**: For each specified customer, the dataset is filtered to include only relevant data for that customer.\n",
      "\n",
      "2. **Separation by Printer**: The filtered data is further separated by individual printers.\n",
      "\n",
      "3. **Cumulative Count of Unique Installations**: Within each printer's data, a cumulative count of unique cartridge installations is calculated. This is achieved by detecting when a new cartridge identifier appears in the data.\n",
      "\n",
      "4. **Marking Changes**: Each change in cartridge identifier (within the same printer and customer group) is marked accordingly.\n",
      "\n",
      "5. **Cumulative Summation**: These changes are then cumulatively summed to compute the total number of makeup cartridges used by each printer over time.\n",
      "\n",
      "6. **Identification of Reused Cartridges**: After establishing the baseline count, cartridges that are reused in more than one printer under the same customer are identified. This is done by filtering for cartridge identifiers that appear across multiple printers, and a flag is set in the dataset to mark such instances as reused.\n",
      "\n",
      "Reference point: The procedure is detailed in the repeated sections of the context provided.\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI  # Make sure this import is correct\n",
    "\n",
    "def answer_query(question, db_path=\"./chroma_store\", similarity_threshold=0.85):\n",
    "    # Step 1: Embed the user question\n",
    "    model = SentenceTransformer(embedding_model)\n",
    "    question_embedding = model.encode([question])[0].tolist()\n",
    "\n",
    "    # Step 2: Load ChromaDB and query relevant document chunks\n",
    "    client_db = PersistentClient(path=db_path)\n",
    "    collection = client_db.get_collection(\"pdf_docs\")\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[question_embedding],\n",
    "        n_results=10,\n",
    "        include=[\"documents\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    documents = results[\"documents\"][0]\n",
    "    distances = results[\"distances\"][0]\n",
    "\n",
    "    # Step 3: Check similarity threshold\n",
    "    if len(distances) == 0 or distances[0] > similarity_threshold:\n",
    "        return \"No idea, this info is not in the documents.\"\n",
    "\n",
    "    context = \"\\n\\n\".join(documents)\n",
    "\n",
    "    # Step 4: Setup the chat messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Answer the question strictly using the context above and also tell the reference point of your answer. If the answer is not found in the context, say: 'No idea, this info is not in the documents.'\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\",\n",
    "            \n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Step 5: Connect to Azure OpenAI\n",
    "    client = AzureOpenAI(\n",
    "        api_key=API_KEY,\n",
    "        api_version=API_VERSION,\n",
    "        azure_endpoint=RESOURCE_ENDPOINT\n",
    "    )\n",
    "\n",
    "    # Step 6: Get response from LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0.2,\n",
    "        presence_penalty=0.1,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "query = 'explain the procedure how the calcualtion of unique make up cartridges is done for each customer'\n",
    "\n",
    "def main(query):\n",
    "    question = query\n",
    "    answer = answer_query(question)\n",
    "    print(\"\\nü§ñ Answer: \")\n",
    "    print(answer)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(query)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
