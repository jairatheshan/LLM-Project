{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb14f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import yaml\n",
    "import nltk\n",
    "import uuid\n",
    "import openai\n",
    "import os\n",
    "import chromadb\n",
    "\n",
    "from chromadb.config import Settings\n",
    "from openai import AzureOpenAI  #\n",
    "from chromadb import PersistentClient\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "from openai import AzureOpenAI  # Make sure this import is correct\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcd5f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(\"Trying to locate 'punkt'...\")\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"‚úÖ 'punkt' is installed correctly.\")\n",
    "except LookupError:\n",
    "    print(\"‚ùå 'punkt' not found. Downloading now...\")\n",
    "    nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d875816",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = \"API_Cred-copy.yaml\"\n",
    "\n",
    "with open(yaml_file_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "# openai\n",
    "API_KEY = config['Open_ai_credentails']['API_KEY']\n",
    "RESOURCE_ENDPOINT = config['Open_ai_credentails']['RESOURCE_ENDPOINT']\n",
    "MODEL = config['Open_ai_credentails']['MODEL']\n",
    "API_VERSION = config['Open_ai_credentails']['API_VERSION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2889d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Extract text from PDF ---\n",
    "def extract_text_from_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "# --- Step 2: Chunk the text into manageable pieces ---\n",
    "def split_text(text, max_chars=500, overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + max_chars\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += max_chars - overlap\n",
    "    return chunks\n",
    "\n",
    "def smart_split_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "\n",
    "def semantic_split_text(text, max_chars=500, overlap=50):\n",
    "    from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(chunk) + len(sentence) <= max_chars:\n",
    "            chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = sentence\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "# --- Step 3: Embed the chunks using SentenceTransformer ---\n",
    "def embed_chunks(chunks, model_name='all-MiniLM-L6-v2'):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(chunks, convert_to_numpy=True)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def store_in_chromadb(chunks, embeddings, db_path=\"./chroma_store\"):\n",
    "    # ‚úÖ Updated client\n",
    "    client = PersistentClient(path=db_path)\n",
    "\n",
    "    # Get or create a collection\n",
    "    collection = client.get_or_create_collection(\"pdf_docs\")\n",
    "\n",
    "    # Add data to the collection\n",
    "    collection.add(\n",
    "        documents=chunks,\n",
    "        embeddings=[e.tolist() for e in embeddings],\n",
    "        metadatas=[{\"source\": \"my_pdf\"} for _ in chunks],\n",
    "        ids=[str(uuid.uuid4()) for _ in chunks]\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Successfully stored into ChromaDB (new API)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff242caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAIN PIPELINE ---\n",
    "pdf_path = \"Documentation of PPI (Customer useage patterns).pdf\"  \n",
    "\n",
    "print(\"üîç Extracting text...\")\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(\"‚úÇÔ∏è Splitting into chunks...\")\n",
    "chunks = semantic_split_text(text)\n",
    "\n",
    "print(\"üß† Embedding chunks...\")\n",
    "embeddings = embed_chunks(chunks)\n",
    "\n",
    "print(\"üíæ Storing into ChromaDB...\")\n",
    "store_in_chromadb(chunks, embeddings)\n",
    "\n",
    "print(\"üöÄ Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdefea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your persisted Chroma vector database\n",
    "client = PersistentClient(path=\"./chroma_store\")\n",
    "# Access the collection you previously created\n",
    "collection = client.get_collection(\"pdf_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b1349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed a user query using the same model used for indexing\n",
    "embedding_model = 'multi-qa-MiniLM-L6-cos-v1' #fast, good for Q&A\n",
    "model = SentenceTransformer(embedding_model)\n",
    "query = \"what is this document about ?\"\n",
    "query_embedding = model.encode([query])[0].tolist()\n",
    "\n",
    "# Search the ChromaDB with that vector\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=5  # top 3 most relevant chunks\n",
    ")\n",
    "\n",
    "# Print out the results\n",
    "print(\"Top matching chunks:\\n\")\n",
    "for doc, metadata in zip(results['documents'][0], results['metadatas'][0]):\n",
    "    print(f\"üìÑ Source: {metadata['source']}\\nüß† Chunk:\\n{doc}\\n{'-'*50}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c0564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI  # Make sure this import is correct\n",
    "\n",
    "def answer_query(question, db_path=\"./chroma_store\", similarity_threshold=0.85):\n",
    "    # Step 1: Embed the user question\n",
    "    model = SentenceTransformer(embedding_model)\n",
    "    question_embedding = model.encode([question])[0].tolist()\n",
    "\n",
    "    # Step 2: Load ChromaDB and query relevant document chunks\n",
    "    client_db = PersistentClient(path=db_path)\n",
    "    collection = client_db.get_collection(\"pdf_docs\")\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[question_embedding],\n",
    "        n_results=10,\n",
    "        include=[\"documents\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    documents = results[\"documents\"][0]\n",
    "    distances = results[\"distances\"][0]\n",
    "\n",
    "    # Step 3: Check similarity threshold\n",
    "    if len(distances) == 0 or distances[0] > similarity_threshold:\n",
    "        return \"No idea, this info is not in the documents.\"\n",
    "\n",
    "    context = \"\\n\\n\".join(documents)\n",
    "\n",
    "    # Step 4: Setup the chat messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Answer the question strictly using the context above and also tell the reference point of your answer. If the answer is not found in the context, say: 'No idea, this info is not in the documents.'\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\",\n",
    "            \n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Step 5: Connect to Azure OpenAI\n",
    "    client = AzureOpenAI(\n",
    "        api_key=API_KEY,\n",
    "        api_version=API_VERSION,\n",
    "        azure_endpoint=RESOURCE_ENDPOINT\n",
    "    )\n",
    "\n",
    "    # Step 6: Get response from LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL\n",
    "        messages=messages,\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0.2,\n",
    "        presence_penalty=0.1,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "query = 'explain the procedure how the calcualtion of unique make up cartridges is done for each customer'\n",
    "\n",
    "def main(query):\n",
    "    question = query\n",
    "    answer = answer_query(question)\n",
    "    print(\"\\nü§ñ Answer: \")\n",
    "    print(answer)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(query)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
